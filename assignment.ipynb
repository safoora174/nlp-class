{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 1\n",
    "Created by Prof. [Mohammad M. Ghassemi](https://ghassemi.xyz)\n",
    "\n",
    "Submitted by: <span style=\"color:red\"> Safoora Masoumi </span>\n",
    "\n",
    "\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Goals\n",
    "The goal of this assignment is to familiarize yourself with:\n",
    "1. Regular expressions, \n",
    "2. Text normalization, \n",
    "3. Edit distance, \n",
    "4. N-gram language models and smoothing \n",
    "\n",
    "This assignment combines tutorial components, with learning exercises that you must complete and submit. The learning exercise sections are clearly demarcated within the assignments.\n",
    "\n",
    "## Assumptions\n",
    "BEFORE YOU START\n",
    "\n",
    "1. PULL THE LATEST VERSION OF THE `course-materials` REPOSITORY, AND COPY `homework/HW1/` INTO THE CORRESPONDING DIRECTORY. \n",
    "2. CREATE AND ATTACHED TO A VIRTUAL ENVIRONMENT, AND INSTALLED THE REQUIREMENTS IN `requirements.txt`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Part 0: Collecting Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can perform any text analysis, we will need to collect some text to apply our analysis to! One excellent resource for free text data is [Project Gutenburg](https://www.gutenberg.org/) (PG); PG contains ~60,000 free eBooks in a variety of formats including the very easy to use `.txt` format. Let's start by pulling [Bertrand Russell's](https://plato.stanford.edu/entries/russell/) book on [The Problems of Philosophy](http://www.gutenberg.org/cache/epub/5827/pg5827.txt) directly from the internet to our machines using [Python's requests library](https://requests.readthedocs.io/en/master/). The requests library is a powerful tool for extracting text data from the internet. Given that most contemporary text data is on the web, it's a tool you should become familiar with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "website = 'http://www.gutenberg.org/cache/epub/5827/pg5827.txt'\n",
    "site    = requests.get(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Server status code: 200\n",
      "--------------------------------------------------------\n",
      "Here is a 250 character sample of your text:\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "wledge in the world which is so certain that no\r\n",
      "reasonable man could doubt it? This question, which at first sight might\r\n",
      "not seem difficult, is really one of the most difficult that can\r\n",
      "be asked. When we have realized the obstacles in the way of a\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Server status code: ' + str(site.status_code))\n",
    "print('--------------------------------------------------------')\n",
    "print('Here is a 250 character sample of your text:')\n",
    "print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n",
    "print(site.text[1500:1750])\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requests object returned a server status code of `200`, and placed the text we requested in `r.text`. The status code is the server's way of telling us `OK` but note that servers can also refuse to serve us; If we request another resources that doesn't exist from Guttenburg for example, it will respond differently. Let's ask for a new book called `idontthinkyouhavethisbook.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "website = 'http://www.gutenberg.org/cache/epub/5827/idontthinkyouhavethisbook.txt'\n",
    "r = requests.get(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Server status code: 404\n",
      "--------------------------------------------------------\n",
      "Here is a 250 character sample of your text:\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "_logo\"> -->\n",
      "  <a id=\"main_logo\" href=\"/\" class=\"no-hover\">\n",
      "    <img src=\"/gutenberg/pg-logo-129x80.png\" alt=\"Project Gutenberg\" draggable=\"false\" />\n",
      "  </a>\n",
      "  <!--\t</div>-->\n",
      "  <div id=\"menu\">\n",
      "    <label for=\"tm\" id=\"toggle-menu\">Menu<span class=\"drop-\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Server status code: ' + str(r.status_code))\n",
    "print('--------------------------------------------------------')\n",
    "print('Here is a 250 character sample of your text:')\n",
    "print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n",
    "print(r.text[1500:1750])\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the server returned a `404` status code, which is it's way of telling us `Not Found`. Sure enough, the content that was returned in `r.text` is the error page of Project Guttenburg, not the book we requested. I point this out because when you collect your own data from the internet, especially large volumes of data, you will want to pay attention to those server status codes to make sure what you requested is what you obtained. To learn more about server codes, you can see this [this page](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "# Part 1: Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regular expression is a compact programming language that is specialized for enhanced text search. We can import the `re` library, which comes standard with the Python programming language, to analyze the book we pulled from Project Guttenburg in part 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_expression = 'philosophy'\n",
    "text               = site.text\n",
    "results            = re.search(regular_expression,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Results\n",
      "--------------------------------------------------------\n",
      "A match for the search query: \"philosophy\"\n",
      "was found at the following character span: (764, 774)\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Results')\n",
    "print('--------------------------------------------------------')\n",
    "print('A match for the search query: \"' + results.group() + '\"\\nwas found at the following character span: ' + str(results.span()))\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example finds the first instance of the word `philosophy` in the book. For some purposes, we may want to capture *every instance* where the search term was mentioned in a text. We can accomplish that using the `re.finditer` function, and packaging our results in a neat list of dictionaries that contain the matching string and the location of the match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the mentions of the search term\n",
    "regular_expression = 'philosophy'\n",
    "results            = [ {\"indicies\":m.span(), \"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "matches            = []; [matches.append(m['match']) for m in results];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Results\n",
      "--------------------------------------------------------\n",
      "We found 72 matches...\n",
      "\n",
      "[{'indicies': (764, 774), 'match': 'philosophy'}, {'indicies': (1833, 1843), 'match': 'philosophy'}, {'indicies': (1849, 1859), 'match': 'philosophy'}, {'indicies': (5109, 5119), 'match': 'philosophy'}, {'indicies': (19891, 19901), 'match': 'philosophy'}, {'indicies': (21170, 21180), 'match': 'philosophy'}, {'indicies': (33483, 33493), 'match': 'philosophy'}, {'indicies': (33560, 33570), 'match': 'philosophy'}, {'indicies': (33849, 33859), 'match': 'philosophy'}, {'indicies': (50321, 50331), 'match': 'philosophy'}, {'indicies': (54760, 54770), 'match': 'philosophy'}, {'indicies': (103297, 103307), 'match': 'philosophy'}, {'indicies': (109118, 109128), 'match': 'philosophy'}, {'indicies': (122400, 122410), 'match': 'philosophy'}, {'indicies': (122529, 122539), 'match': 'philosophy'}, {'indicies': (125952, 125962), 'match': 'philosophy'}, {'indicies': (126061, 126071), 'match': 'philosophy'}, {'indicies': (130439, 130449), 'match': 'philosophy'}, {'indicies': (137039, 137049), 'match': 'philosophy'}, {'indicies': (137797, 137807), 'match': 'philosophy'}, {'indicies': (141982, 141992), 'match': 'philosophy'}, {'indicies': (142856, 142866), 'match': 'philosophy'}, {'indicies': (143182, 143192), 'match': 'philosophy'}, {'indicies': (187623, 187633), 'match': 'philosophy'}, {'indicies': (217060, 217070), 'match': 'philosophy'}, {'indicies': (217606, 217616), 'match': 'philosophy'}, {'indicies': (218230, 218240), 'match': 'philosophy'}, {'indicies': (218493, 218503), 'match': 'philosophy'}, {'indicies': (231308, 231318), 'match': 'philosophy'}, {'indicies': (231368, 231378), 'match': 'philosophy'}, {'indicies': (231474, 231484), 'match': 'philosophy'}, {'indicies': (232593, 232603), 'match': 'philosophy'}, {'indicies': (233131, 233141), 'match': 'philosophy'}, {'indicies': (233425, 233435), 'match': 'philosophy'}, {'indicies': (233545, 233555), 'match': 'philosophy'}, {'indicies': (233816, 233826), 'match': 'philosophy'}, {'indicies': (234323, 234333), 'match': 'philosophy'}, {'indicies': (234555, 234565), 'match': 'philosophy'}, {'indicies': (235193, 235203), 'match': 'philosophy'}, {'indicies': (235369, 235379), 'match': 'philosophy'}, {'indicies': (235447, 235457), 'match': 'philosophy'}, {'indicies': (235666, 235676), 'match': 'philosophy'}, {'indicies': (235850, 235860), 'match': 'philosophy'}, {'indicies': (235992, 236002), 'match': 'philosophy'}, {'indicies': (236350, 236360), 'match': 'philosophy'}, {'indicies': (236378, 236388), 'match': 'philosophy'}, {'indicies': (236439, 236449), 'match': 'philosophy'}, {'indicies': (236606, 236616), 'match': 'philosophy'}, {'indicies': (236725, 236735), 'match': 'philosophy'}, {'indicies': (237387, 237397), 'match': 'philosophy'}, {'indicies': (237505, 237515), 'match': 'philosophy'}, {'indicies': (237868, 237878), 'match': 'philosophy'}, {'indicies': (238560, 238570), 'match': 'philosophy'}, {'indicies': (238691, 238701), 'match': 'philosophy'}, {'indicies': (238775, 238785), 'match': 'philosophy'}, {'indicies': (238849, 238859), 'match': 'philosophy'}, {'indicies': (238890, 238900), 'match': 'philosophy'}, {'indicies': (238988, 238998), 'match': 'philosophy'}, {'indicies': (239235, 239245), 'match': 'philosophy'}, {'indicies': (239324, 239334), 'match': 'philosophy'}, {'indicies': (239976, 239986), 'match': 'philosophy'}, {'indicies': (240137, 240147), 'match': 'philosophy'}, {'indicies': (240278, 240288), 'match': 'philosophy'}, {'indicies': (240616, 240626), 'match': 'philosophy'}, {'indicies': (241278, 241288), 'match': 'philosophy'}, {'indicies': (241368, 241378), 'match': 'philosophy'}, {'indicies': (241511, 241521), 'match': 'philosophy'}, {'indicies': (241613, 241623), 'match': 'philosophy'}, {'indicies': (242858, 242868), 'match': 'philosophy'}, {'indicies': (249062, 249072), 'match': 'philosophy'}, {'indicies': (249535, 249545), 'match': 'philosophy'}, {'indicies': (249777, 249787), 'match': 'philosophy'}]\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Results')\n",
    "print('--------------------------------------------------------')\n",
    "print('We found ' +  str(len(matches)) + ' matches...\\n')\n",
    "print(str(results))\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found `72` matches for the search term. That seems a bit low for a book with the word philosophy in it's name! Let's update our regular expression so that it's not sensitive to capitalization of the letter `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the mentions of the search term\n",
    "regular_expression = '(p|P)hilosophy'\n",
    "results            = [ {\"indicies\":m.span(), \"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "matches            = []; [matches.append(m['match']) for m in results];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Results\n",
      "--------------------------------------------------------\n",
      "We found 81 matches...\n",
      "\n",
      "['Philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'Philosophy']\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Results')\n",
    "print('--------------------------------------------------------')\n",
    "print('We found ' +  str(len(results)) + ' matches...\\n')\n",
    "print(str(matches))\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found `81` matches; that is not much better than before. Let's make the search even more general by seeking out terms that start with the word `philo`, followed by an arbitrary number of [a-z] characters thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_expression = '(P|p)hilo[a-z]+'\n",
    "results            = [ {\"indicies\":m.span(), \"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "matches            = []; [matches.append(m['match']) for m in results];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Results\n",
      "--------------------------------------------------------\n",
      "We found 154 matches...\n",
      "\n",
      "['Philosophy', 'Philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophy', 'philosophy', 'philosopher', 'philosopher', 'philosopher', 'Philonous', 'Philonous', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'Philosophy', 'philosophy', 'philosophy', 'philosopher', 'philosophical', 'Philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophy', 'philosophical', 'philosopher', 'philosophical', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'philosophy', 'philosophical', 'philosopher', 'philosophy', 'philosopher', 'philosophy', 'philosopher', 'philosophy', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'philosopher', 'philosophers', 'philosophy', 'philosophy', 'philosophical', 'philosophers', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophers', 'philosophers', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophers', 'philosophies', 'philosophers', 'philosophical', 'philosopher', 'philosophers', 'philosophers', 'philosophers', 'philosophy', 'philosophical', 'philosophy', 'philosophers', 'philosophers', 'philosophy', 'philosophy', 'philosophy', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'Philosophical', 'philosophy', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophical', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosopher', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophical', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophize', 'Philosophy', 'philosophy', 'philosophic', 'philosophic', 'Philosophic', 'Philosophic', 'philosophic', 'philosophies', 'philosophical', 'philosophic', 'philosophic', 'philosophic', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophers', 'Philonous', 'Philosophy']\n",
      "--------------------------------------------------------\n",
      "We found 11 distinct terms...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-0344fe33d48b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--------------------------------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'We found '\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' distinct terms...\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCountFrequency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--------------------------------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_utils' is not defined"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Results')\n",
    "print('--------------------------------------------------------')\n",
    "print('We found ' +  str(len(matches)) + ' matches...\\n')\n",
    "print(str(matches))\n",
    "print('--------------------------------------------------------')\n",
    "print('We found ' +  str(len(set(matches))) + ' distinct terms...\\n')\n",
    "print(class_utils.CountFrequency(matches))\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better! But now we have a new problem. It seems that an inconveniently named philosopher, `Philonous`, has made his way into our results. Let's modify our regular expression to use a `negative lookahead` `(?!nous)` to remove him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_expression = '(P|p)hilo(?!nous)[a-z]+'\n",
    "results            = [ {\"indicies\":m.span(), \"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "matches            = []; [matches.append(m['match']) for m in results];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Results\n",
      "--------------------------------------------------------\n",
      "We found 151 matches...\n",
      "\n",
      "['Philosophy', 'Philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophy', 'philosophy', 'philosopher', 'philosopher', 'philosopher', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'Philosophy', 'philosophy', 'philosophy', 'philosopher', 'philosophical', 'Philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophy', 'philosophical', 'philosopher', 'philosophical', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'philosophy', 'philosophical', 'philosopher', 'philosophy', 'philosopher', 'philosophy', 'philosopher', 'philosophy', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'philosopher', 'philosophers', 'philosophy', 'philosophy', 'philosophical', 'philosophers', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophers', 'philosophers', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophers', 'philosophies', 'philosophers', 'philosophical', 'philosopher', 'philosophers', 'philosophers', 'philosophers', 'philosophy', 'philosophical', 'philosophy', 'philosophers', 'philosophers', 'philosophy', 'philosophy', 'philosophy', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'Philosophical', 'philosophy', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophical', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosopher', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophical', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophize', 'Philosophy', 'philosophy', 'philosophic', 'philosophic', 'Philosophic', 'Philosophic', 'philosophic', 'philosophies', 'philosophical', 'philosophic', 'philosophic', 'philosophic', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophers', 'Philosophy']\n",
      "--------------------------------------------------------\n",
      "We found 10 distinct terms...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-0344fe33d48b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--------------------------------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'We found '\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' distinct terms...\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCountFrequency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--------------------------------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_utils' is not defined"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Results')\n",
    "print('--------------------------------------------------------')\n",
    "print('We found ' +  str(len(matches)) + ' matches...\\n')\n",
    "print(str(matches))\n",
    "print('--------------------------------------------------------')\n",
    "print('We found ' +  str(len(set(matches))) + ' distinct terms...\\n')\n",
    "print(class_utils.CountFrequency(matches))\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 1: \n",
    "#### Worth 1/5 Points\n",
    "Now I'm curious how Bertrand used these various philosophical terms in his sentences, and how Bertrand's language, more generally, compares to [Friedrich Nietzsche](https://en.wikipedia.org/wiki/Friedrich_Nietzsche) and other philosophers. You're going to help me with that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Download Data\n",
    "Use Python's `requests` library to collect Friedrich Nietzsche's book, [Beyond Good and Evil](http://www.gutenberg.org/cache/epub/4363/pg4363.txt) into Python. Collect one additional work of philosophy (of your choice) and import it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "website0 = 'http://www.gutenberg.org/cache/epub/4363/pg4363.txt'\n",
    "site0    = requests.get(website0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Server status code: 200\n",
      "--------------------------------------------------------\n",
      "Here is a 250 character sample of your text:\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "antiquity, Plato? Had the wicked\r\n",
      "Socrates really corrupted him? Was Socrates after all a corrupter of\r\n",
      "youths, and deserved his hemlock?\" But the struggle against Plato,\r\n",
      "or--to speak plainer, and for the \"people\"--the struggle against\r\n",
      "the ecclesia\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Server status code: ' + str(site0.status_code))\n",
    "print('--------------------------------------------------------')\n",
    "print('Here is a 250 character sample of your text:')\n",
    "print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n",
    "print(site0.text[5000:5250])\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "website1 = 'https://www.gutenberg.org/cache/epub/2680/pg2680.txt'\n",
    "site1    = requests.get(website1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Server status code: 200\n",
      "--------------------------------------------------------\n",
      "Here is a section of the Great Emperor Marcus Aurelius Antoninus Meditations:\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "In Rome itself there\r\n",
      "was pestilence and starvation, the one brought from the east by Verus's\r\n",
      "legions, the other caused by floods which had destroyed vast quantities\r\n",
      "of grain. After all had been done possible to allay famine and to supply\r\n",
      "pressing needs--Marcus being forced even to sell the imperial jewels to\r\n",
      "find money--both emperors set forth to a struggle which was to continue\r\n",
      "more or less during the rest of Marcus's reign. During these wars, in\r\n",
      "169, Verus died. We have no means of following the campaigns in detail;\r\n",
      "but thus much is certain, that in the end the Romans succeeded in\r\n",
      "crushing the barbarian tribes, and effecting a settlement which made the\r\n",
      "empire more se\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Server status code: ' + str(site1.status_code))\n",
    "print('--------------------------------------------------------')\n",
    "print('Here is a section of the Great Emperor Marcus Aurelius Antoninus Meditations:')\n",
    "print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n",
    "print(site1.text[4913:5600])\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Basic Text Cleansing\n",
    "Use `re.sub` to do some basic text cleansing on books you downloaded. More specifically: \n",
    "\n",
    "1. replace the various forms of whitespace that appear (`\\n`,`\\r`,`\\t` etc.) with a single whitespace character. \n",
    "2. cast all text to lower case\n",
    "3. replace common abbreviations with their full form, for instance: `it's` with `it is`\n",
    "\n",
    "Note that item #3 does not have to be comprehensive, just do a couple common abbreviations to illustrate that you know how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "textWS=re.sub(\"\\s+\", \" \", site1.text)\n",
    "ltext=textWS.lower()\n",
    "#print(ltext)\n",
    "abb=re.sub(r\"won\\'t\", \"will not\", ltext)\n",
    "abb1=re.sub(r\"it\\'s\", \"it is\", abb)\n",
    "abb2=re.sub(r\"\\'s\", \" is\", abb1)\n",
    "abb3=re.sub(r\"\\'ll\", \" will\", abb2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "textws=re.sub(\"\\s+\", \" \", site0.text)\n",
    "texts=textws.lower()\n",
    "abbs=re.sub(r\"\\ve\", \" have\", texts)\n",
    "abb4=re.sub(r\"it\\'s\", \"it is\", abbs)\n",
    "abb5=re.sub(r\"can\\'t\", \"cannot\", abb4)\n",
    "abb6=re.sub(r\"\\'t\", \" not\", abb5)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "At first I removed each whitespace and replace it with a single one.Then I convert the text into lowercase and replace some of the abbreviation with full term,however i could not find many abbs in the context of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Sentence Extractor\n",
    "Write a regular expression that will extract the complete sentences containing any matches of the following regex we wrote together earlier: `(P|p)hilo(?!nous)[a-z]+`. Apply this regular expression to \"Beyond Good and Evil\", \"The Problems of Philosophy\", and the work of philosophy you chose. Show a couple of example sentences from each work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Problems of Philosophy: [' but it cannot be maintained that philosophy has had any very great measure of success in its attempts to provide definite answers to its questions.', ' but if you put the same question to a philosopher, he will, if he is candid, have to confess that his study has not achieved positive results such as have been achieved by other sciences.', ' it is true that this is partly accounted for by the fact that, as soon as definite knowledge concerning any subject becomes possible, this subject ceases to be called philosophy, and becomes a separate science.']\n"
     ]
    }
   ],
   "source": [
    "regular_expression= \"[^.]*(P|p)hilo(?!nous)[a-z][^.]*\\.\"\n",
    "text=site.text\n",
    "text=(re.sub(\"\\s+\", \" \", site.text)).lower()\n",
    "results = [ {\"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "matches= []; [matches.append(m['match']) for m in results];\n",
    "print(\"The Problems of Philosophy:\",matches[96:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyond Good and Evil: [' prejudices of philosophers 1.', ' the will to truth, which is to tempt us to many a hazardous enterprise, the famous truthfulness of which all philosophers have hitherto spoken with respect, what questions has this will to truth not laid before us! what strange, perplexing, questionable questions! it is already a long story; yet it seems as if it were hardly commenced.', ' perhaps! but who wishes to concern himself with such dangerous \"perhapses\"! for that investigation one must await the advent of a new order of philosophers, such as will have other tastes and inclinations, the reverse of those hitherto prevalent--philosophers of the dangerous \"perhaps\" in every sense of the term.']\n"
     ]
    }
   ],
   "source": [
    "regular_expression= \"[^.]*(P|p)hilo(?!nous)[a-z][^.]*\\.\"\n",
    "text=site0.text\n",
    "text=(re.sub(\"\\s+\", \" \", site0.text)).lower()\n",
    "results = [ {\"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "matches0= []; [matches0.append(m['match']) for m in results];\n",
    "print( \"Beyond Good and Evil:\",matches0[5:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Meditations, by Marcus Aurelius: [' antisthenes of athens, founder of the sect of cynic philosophers, and an opponent of plato, 5th century b.', ', a stoic philosopher, and the founder of stoicism as a systematic philosophy.', ' crates, a cynic philosopher of the 4th century b.']\n"
     ]
    }
   ],
   "source": [
    "regular_expression= \"[^.]*(P|p)hilo(?!nous)[a-z][^.]*\\.\"\n",
    "text=site1.text\n",
    "text=(re.sub(\"\\s+\", \" \", site1.text)).lower()\n",
    "results = [ {\"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "\n",
    "matches1= []; [matches1.append(m['match']) for m in results];\n",
    "print( \" Meditations, by Marcus Aurelius:\",matches1[44:47])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Sentence Tokenizer\n",
    "Write another regular expression that tokenizes the text in your collected sentences by splitting the sentences into a list of individual words. Tokenize the entire text of all three books using this regular expression. For each book, store your result as a list of lists and print the last word of the last sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The last word of the last sentence: 5827-8. \n",
      "\n",
      "['but', 'it', 'cannot', 'be', 'maintained', 'that', 'philosophy', 'has', 'had', 'any', 'very', 'great', 'measure', 'of', 'success', 'in', 'its', 'attempts', 'to', 'provide', 'definite', 'answers', 'to', 'its', 'questions.', 'but', 'if', 'you', 'put', 'the', 'same', 'question', 'to', 'a', 'philosopher,', 'he', 'will,', 'if', 'he', 'is', 'candid,', 'have', 'to', 'confess', 'that', 'his', 'study', 'has', 'not', 'achieved', 'positive', 'results', 'such', 'as', 'have', 'been', 'achieved', 'by', 'other', 'sciences.', 'it', 'is', 'true', 'that', 'this', 'is', 'partly', 'accounted', 'for', 'by', 'the', 'fact', 'that,', 'as', 'soon', 'as', 'definite', 'knowledge', 'concerning', 'any', 'subject', 'becomes', 'possible,', 'this', 'subject', 'ceases', 'to', 'be', 'called', 'philosophy,', 'and', 'becomes', 'a', 'separate', 'science.']\n"
     ]
    }
   ],
   "source": [
    "#for The Problems of Philosophy\n",
    "import nltk\n",
    "regular_expression= \"[^.]*(P|p)hilo(?!nous)[a-z][^.]*\\.\"\n",
    "text=(re.sub(\"\\s+\", \" \", site.text)).lower()\n",
    "results = [ {\"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "match= []; [match.append(m['match']) for m in results];\n",
    "#for the whole book\n",
    "def totaltoken(match):\n",
    "    totaltoken=[]\n",
    "    for i in match:\n",
    "        token =nltk.WhitespaceTokenizer().tokenize(i)\n",
    "        totaltoken.append(token)\n",
    "    return totaltoken\n",
    "#print(totaltoken(match))\n",
    "print(\"\\nThe last word of the last sentence:\",totaltoken(match)[-1][-1],\"\\n\")\n",
    "\n",
    "#one list contains the tokenized selected sentences\n",
    "finalList = []\n",
    "for i in totaltoken(match[96:99]):\n",
    "    finalList.extend(i)\n",
    "print(finalList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The last word of the last sentence: labyrinths. \n",
      "\n",
      "['prejudices', 'of', 'philosophers', '1.', 'the', 'will', 'to', 'truth,', 'which', 'is', 'to', 'tempt', 'us', 'to', 'many', 'a', 'hazardous', 'enterprise,', 'the', 'famous', 'truthfulness', 'of', 'which', 'all', 'philosophers', 'have', 'hitherto', 'spoken', 'with', 'respect,', 'what', 'questions', 'has', 'this', 'will', 'to', 'truth', 'not', 'laid', 'before', 'us!', 'what', 'strange,', 'perplexing,', 'questionable', 'questions!', 'it', 'is', 'already', 'a', 'long', 'story;', 'yet', 'it', 'seems', 'as', 'if', 'it', 'were', 'hardly', 'commenced.', 'perhaps!', 'but', 'who', 'wishes', 'to', 'concern', 'himself', 'with', 'such', 'dangerous', '\"perhapses\"!', 'for', 'that', 'investigation', 'one', 'must', 'await', 'the', 'advent', 'of', 'a', 'new', 'order', 'of', 'philosophers,', 'such', 'as', 'will', 'have', 'other', 'tastes', 'and', 'inclinations,', 'the', 'reverse', 'of', 'those', 'hitherto', 'prevalent--philosophers', 'of', 'the', 'dangerous', '\"perhaps\"', 'in', 'every', 'sense', 'of', 'the', 'term.']\n"
     ]
    }
   ],
   "source": [
    "#Beyond Good and Evil,\n",
    "import nltk\n",
    "regular_expression= \"[^.]*(P|p)hilo(?!nous)[a-z][^.]*\\.\"\n",
    "text=(re.sub(\"\\s+\", \" \", site0.text)).lower()\n",
    "results = [ {\"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "match= []; [match.append(m['match']) for m in results];\n",
    "#for the whole book\n",
    "def totaltoken(match):\n",
    "    totaltoken=[]\n",
    "    for i in match:\n",
    "        token =nltk.WhitespaceTokenizer().tokenize(i)\n",
    "        totaltoken.append(token)\n",
    "    return totaltoken\n",
    "#print(totaltoken(match))\n",
    "print(\"\\nThe last word of the last sentence:\",totaltoken(match)[-1][-1],\"\\n\")\n",
    "\n",
    "#one list contains the tokenized selected sentences\n",
    "finalList = []\n",
    "for i in totaltoken(match[5:8]):\n",
    "    finalList.extend(i)\n",
    "print(finalList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The last word of the last sentence: academy. \n",
      "\n",
      "['antisthenes', 'of', 'athens,', 'founder', 'of', 'the', 'sect', 'of', 'cynic', 'philosophers,', 'and', 'an', 'opponent', 'of', 'plato,', '5th', 'century', 'b.', ',', 'a', 'stoic', 'philosopher,', 'and', 'the', 'founder', 'of', 'stoicism', 'as', 'a', 'systematic', 'philosophy.', 'crates,', 'a', 'cynic', 'philosopher', 'of', 'the', '4th', 'century', 'b.']\n"
     ]
    }
   ],
   "source": [
    "#\" Meditations, by Marcus Aurelius:\"\n",
    "import nltk\n",
    "regular_expression= \"[^.]*(P|p)hilo(?!nous)[a-z][^.]*\\.\"\n",
    "text=(re.sub(\"\\s+\", \" \", site1.text)).lower()\n",
    "results = [ {\"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "match= []; [match.append(m['match']) for m in results];\n",
    "#for the whole book\n",
    "def totaltoken(match):\n",
    "    totaltoken=[]\n",
    "    for i in match:\n",
    "        token =nltk.WhitespaceTokenizer().tokenize(i)\n",
    "        totaltoken.append(token)\n",
    "    return totaltoken\n",
    "#print(totaltoken(match))\n",
    "print(\"\\nThe last word of the last sentence:\",totaltoken(match)[-1][-1],\"\\n\")\n",
    "\n",
    "#one list contains the tokenized selected sentences\n",
    "finalList = []\n",
    "for i in totaltoken(match[44:47]):\n",
    "    finalList.extend(i)\n",
    "print(finalList)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I did not know if I can use other fuction of Tokenizing or not.I used https://www.nltk.org/_modules/nltk/tokenize/regexp.html\n",
    "and  https://www.nltk.org/_modules/nltk/tokenize/treebank.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Compare Works\n",
    "Use Python's `matplotlib` library, to compare the distribution of the most common words that show up in each book. Compare the distributions and comment on any important statistical similarities or differences between the distributions. Do the distributions of the word frequency follow Zipf's law? Ignoring common words (`the`,`and`,`in`, etc.) what are the top 10 words that each book used most frequently? Are there any words that showed up more frequently in one work than they did in another?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('knowledge', 303), ('may', 173), ('must', 169), ('things', 167), ('know', 160), ('one', 156), ('us', 152), ('thus', 146), ('two', 135), ('true', 129)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':' 10 most common words'}, xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The Problems of Philosophy\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib\n",
    "from nltk.probability import FreqDist\n",
    "stopwords.words('english')\n",
    "#print(stopwords.words('english'))\n",
    "text=(re.sub(\"\\s+\", \" \", site.text)).lower()\n",
    "token =nltk.word_tokenize(text)\n",
    "tokens_wosw = [word for word in token if not word in stopwords.words('english')]\n",
    "words = [word for word in tokens_wosw if word.isalpha()]\n",
    "fdist_filtered = FreqDist(words)\n",
    "top = fdist_filtered.most_common()\n",
    "print(top[:10])\n",
    "fdist_filtered.plot(10,title=' 10 most common words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('one', 445), ('man', 239), ('even', 194), ('also', 166), ('perhaps', 158), ('every', 143), ('us', 138), ('good', 133), ('must', 132), ('may', 131)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':' 10 most common words'}, xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Beyond Good and Evil,\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib\n",
    "from nltk.probability import FreqDist\n",
    "stopwords.words('english')\n",
    "#print(stopwords.words('english'))\n",
    "text=(re.sub(\"\\s+\", \" \", site0.text)).lower()\n",
    "token =nltk.word_tokenize(text)\n",
    "tokens_wosw = [word for word in token if not word in stopwords.words('english')]\n",
    "words = [word for word in tokens_wosw if word.isalpha()]\n",
    "fdist_filtered = FreqDist(words)\n",
    "top = fdist_filtered.most_common()\n",
    "print(top[:10])\n",
    "fdist_filtered.plot(10,title=' 10 most common words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thou', 818), ('things', 554), ('unto', 439), ('man', 368), ('thy', 347), ('one', 338), ('thee', 312), ('nature', 273), ('doth', 252), ('either', 239)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':' 10 most common words'}, xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\" Meditations, by Marcus Aurelius:\"\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib\n",
    "from nltk.probability import FreqDist\n",
    "stopwords.words('english')\n",
    "#print(stopwords.words('english'))\n",
    "text=(re.sub(\"\\s+\", \" \", site1.text)).lower()\n",
    "token =nltk.word_tokenize(text)\n",
    "tokens_wosw = [word for word in token if not word in stopwords.words('english')]\n",
    "words = [word for word in tokens_wosw if word.isalpha()]\n",
    "fdist_filtered = FreqDist(words)\n",
    "top = fdist_filtered.most_common()\n",
    "print(top[:10])\n",
    "fdist_filtered.plot(10,title=' 10 most common words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "# Part 2: Text normalization \n",
    "Text normalization is an essential preliminary step to any natural language processing pipeline. To begin, let's re-import `The Problems of Philosophy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "website  = 'http://www.gutenberg.org/cache/epub/5827/pg5827.txt'\n",
    "text     = requests.get(website).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "As you saw in `Learning Exercise 1`, tokenization of text can be performed through the use of Regular Expressions. Familiarity with Regular Expressions is an important part of any NLP apprentice's toolkit but fortunately, it's not the only (or even the best) tool that exists for text normalization tasks! Indeed, for basic text normalization and cleansing, Python has a number of functions that operate on strings directly - no libraries required! \n",
    "\n",
    "Let's begin this section of the tutorial by taking a look at some of these functions, starting with the `replace` function to remove the whitespace characters in our text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace \\r\\n with whitespace, then replace \\n with whitespace, then replace \\r with whitespace\n",
    "clean_text = text.replace('\\r\\n',' ').replace('\\n',' ').replace('\\r',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Results\n",
      "--------------------------------------------------------\n",
      "Before:\n",
      "\r\n",
      "\r\n",
      "I have derived valuable assistance from unpublished writings of G. E.\r\n",
      "Moore and J. M. Keynes: from the former, as regards ...\n",
      "--------------------------------------------------------\n",
      "After:\n",
      "I have derived valuable assistance from unpublished writings of G. E. Moore and J. M. Keynes: from the former, as regards ...\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Results')\n",
    "print('--------------------------------------------------------')\n",
    "print('Before:\\n' + text[1101:1227] + ' ...')\n",
    "print('--------------------------------------------------------')\n",
    "print('After:\\n' + clean_text[1056:1177] + ' ...')\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to have worked! Notice that `replace` is a method that is built into Python strings. That's the reason that we write `text.replace(...)` and not something like `replace(text,...)`.  \n",
    "\n",
    "Python has other many [other useful string manipulation methods](https://www.programiz.com/python-programming/methods/string), but for the rest of the basic pre-processing we'll be used in this tutorial, we'll only need `split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sentences when we see a '.' character\n",
    "clean_text = clean_text.split('.')\n",
    "\n",
    "# Split each sentence further - when there are no arguments provided to split, it defaults to whitespace (tabs, newlines, spaces, etc)\n",
    "textmat = []\n",
    "for c in clean_text:\n",
    "    textmat.append(c.split())    \n",
    "#re.split(r'\\w+|\\$[\\d\\.]+|\\S+',\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Results (showing sentence 10)\n",
      "--------------------------------------------------------\n",
      "['I', 'have', 'also', 'profited', 'greatly', 'by', 'the', 'criticisms', 'and', 'suggestions', 'of', 'Professor', 'Gilbert', 'Murray']\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the tenth sentence:\n",
    "print('--------------------------------------------------------')\n",
    "print('Results (showing sentence 10)')\n",
    "print('--------------------------------------------------------')\n",
    "print(textmat[10])\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing shown above is mostly for illustrative purposes. If we'd like to do anything serious, we'll need a more sophisticated approach to sentence segmentation and text tokenization. Fortunately, there are many existing tools in Python that assist with the Sentence segmentation and word tokenization. One of the most popular tools for this is the [Natural Langauge Toolkit](https://www.nltk.org/). Let's import the tool into python and explore some of it's functionality together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the NLTK Package\n",
    "import nltk\n",
    "\n",
    "# Importing the `punkt` data, used by the NLTK tokenizer\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test out the NLTK tokenizer with a tricky sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.', 'He', 'drove', 'to', 'the', 'store', 'in', 'New', 'York', 'at', '100', 'm.p.h.', '!', 'He', 'wanted', 'to', 'obtain', 'some', 'Kool-Aid', '!', 'It', 'cost', '$', '10.89', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good. He drove to the store in New York at 100 m.p.h.! He wanted to obtain some Kool-Aid! It cost $10.89.\"\"\"\n",
    "tokens    = nltk.word_tokenize(sentences)\n",
    "print('\\n')\n",
    "print(tokens)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tokenization is pretty good for something out-of-the box, but it could be better! In an ideal world, we would want the Tokenizer to understand that `New York` is actually a single token, not two. Furthermore, depending on our use case, we may want the `$` sign to be attached to the `10.89` token. So, if you wanted to use the NLTK tool out-of-the-box, you may need to do some additional pre-processing. Let's try the sentence splitting functionality next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[\"At eight o'clock on Thursday morning Arthur didn't feel very good.\", 'He drove to the store in New York at 100 m.p.h.!', 'He wanted to obtain some Kool-Aid!', 'It cost $10.89.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = nltk.sent_tokenize(sentences)\n",
    "print('\\n' + str(result) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one also works rather nicely. Indeed, NLTK does a lot to make Natural Language Processing accessible. In fact, NLTK has several other tools that we will cover in subsequent lectures, but one of the tools that is not yet available in NLTK (at the time this tutorial was written) is Byte Pair Encoding, the tokenization approaches with some relation to many of today's NLP powerhouses (Yes, I'm talking about [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))). I think I'm going to need some help with that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 2:\n",
    "#### Worth 1/5 Points\n",
    "\n",
    "For this learning exercise, you will implement the Byte Pair Encoding (BPE) algorithm and use it to Tokenize `The Problems of Philosophy`. If you can't recall how the BPE algorithm works, you can refer back to the lectures, or see Section 2.4.3 of [Daniel Jurafsky & James H. Martin's NLP book](https://web.stanford.edu/~jurafsky/slp3/2.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Download Books to Create a Larger Corpus\n",
    "Given that the BPE algorithm is data-driven, we'll first need to gather a training data corpus. For your training data, you will choose at least 10 books from [Project Gutenburg](https://www.gutenberg.org/): \n",
    "* Choose at least 5 books that were authored by Bertrand Russell, not including `The Problems of Philosophy`\n",
    "* Choose at least 5 books that were authored by Friedrich Nietzsche\n",
    "\n",
    "You'll be using these training data later with BPE to identify the token `vocabularies`, which you will then use to tokenize `The problems of Philosophy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "#Bertrand Russell\n",
    "\n",
    "website = ['https://www.gutenberg.org/files/55610/55610-0.txt',\n",
    "           'https://www.gutenberg.org/cache/epub/4776/pg4776.txt',\n",
    "           'https://www.gutenberg.org/files/37090/37090-0.txt',\n",
    "           'https://www.gutenberg.org/cache/epub/690/pg690.txt',\n",
    "           'https://www.gutenberg.org/cache/epub/2529/pg2529.txt']\n",
    "\n",
    "\n",
    "#Friedrich Nietzsche\n",
    "websiteNietzsche = ['https://www.gutenberg.org/cache/epub/19322/pg19322.txt',\n",
    "                    'https://www.gutenberg.org/files/52915/52915-0.txt',\n",
    "                    'https://www.gutenberg.org/files/52190/52190-0.txt',\n",
    "                    'https://www.gutenberg.org/files/37841/37841-0.txt',\n",
    "                    'https://www.gutenberg.org/cache/epub/5652/pg5652.txt']\n",
    "def trainingdata(website):\n",
    "    for i in website:\n",
    "        site=requests.get(i)\n",
    "        texts=(re.sub(\"\\s+\", \" \", site.text)).lower()\n",
    "        return texts\n",
    "\n",
    "\n",
    "#print(trainingdata(website))\n",
    "\n",
    "#token =nltk.word_tokenize(texts)\n",
    "#(' '.join(create_new_sentence(15, 'keeps')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Implement Byte Pair Encoding\n",
    "Implement the Byte Pair Encoding algorithm by filling out the function skeleton below. The function should take three inputs:\n",
    "\n",
    "* `text` : an unprocessed text file that we wish to tokenize using BPE\n",
    "* `k`    : the number of merges to perform\n",
    "* `training_corpus` : a python list of complete texts we wish to use for training. The formatting should look like: `['blah blah blah ...', 'the rain in spain ...', 'do pickles count as vegetables? ...']`\n",
    "\n",
    "The function should output a tokenized version of the `text` based on the `k` vocabulary units learned by BPE using the `training_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "text=trainingdata(website)\n",
    "\n",
    "def vocabulary(text):\n",
    "    tokens = [\" \".join(word) + \" </w>\" for word in text.split()]\n",
    "    vocab = Counter(tokens)  \n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, frequency in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += frequency\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):    \n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "num_merges = 10  #k\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "\n",
    "    \n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "to be honest i did not write this code myself ,i read it on a website and and try to understand it and then re write it.Understanding the concept of BPE is difficult for me. I dont know the initial vocabulary consists of letters or words. The image in the book confused me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Evaluate Byte Pair Encoding\n",
    "Because BPE is data-driven, we might expect there to be some differences in the tokenization based on the training corpus, as well as your choice of `k`! To evaluate BPE, you will now experiment with how the tokenization performs for various settings of `k`, and when using different training . More specifically, you will compare the `vocabulary` and `tokenized_text` of Bertrand Russell's `The Problem's of Philosophy` for:\n",
    "\n",
    "* values of `k` ranging from 1,000 to 10,000 in steps of 1,000 and\n",
    "    *  `training_corpus` sets that include: (1) only Bertrand Russell books, (2) only Friedrich Nietzsche books, and (3) all books from groups (1) and (2).\n",
    "\n",
    "For the above conditions, please generate one or more plots using Python's `matplotlib` that compare:\n",
    "* The total number of unique tokens in the tokenized text\n",
    "* The median size of tokens in the tokenized text\n",
    "* The % overlap in the number of distinct unique tokens generated by BPE and NLTK's word tokenizer.\n",
    "\n",
    "Compare the plots, and note any interesting observations about the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = 'http://www.gutenberg.org/cache/epub/5827/pg5827.txt'\n",
    "site    = requests.get(website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "# Part 3: Edit Distance\n",
    "#### Character Level Distance\n",
    "As we discussed in the lectures, Edit Distance is a way to measure the similarity between different sequences of text. Our goal in this section will be to compare and contrast some of the different measures of edit distance that exist so that you'll have a better intuition for how to use them, depending on the problem you're trying to solve. To start, let's import the [`textdistance` library](https://github.com/life4/textdistance), written by [Gram Voronov](https://orsinium.dev/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `textdistance` library contains python implementations of about 30 edit distance algorithms. Let's begin by trying out the edit distance tool we discussed in class, the [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "levenshtein distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 0\n",
      "\"test\" and \"texts\" is: 2\n",
      "\"test\" and \"test   \" is: 3\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def GetDistance(string1, string2, method):\n",
    "    # Compute the distance and print the results\n",
    "    distance = eval(\"textdistance.\" + method + \"(string1, string2)\")\n",
    "    print('\"' + string1 + '\" and \"' + string2 + '\" is: ' + str(distance))  \n",
    "    \n",
    "print('\\n')\n",
    "print('levenshtein' + \" distance:\")\n",
    "print('--------------------------------------------------------')\n",
    "GetDistance('test','test'      ,'levenshtein')\n",
    "GetDistance('test','texts'     ,'levenshtein')\n",
    "GetDistance('test','test   '   ,'levenshtein')\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture that the Levenshtein distance counts the total number of `insertion`, `deletion`, and `substitution` actions that a required to transform one string into another and the cost of each edit is always 1. So, as shown in the above example, moving from `\"test\"` to `\"test\"` requires no edits, and therefore has a distance of 0, moving from `\"test\"` to `\"texts\"` requires 1 substitution and one addition and therefore has a distance of 2, moving from `\"test\"` to `\"   test\"`   has a distance of 3 becasue we must remove the  three whitespace characters to the left of the word `\"   test\"` to create a match.\n",
    "\n",
    "The above example is intentionally meant to highlight that the assumptions of the edit distance measures can have consequence for their estimates; Indeed, depending on the problem we are trying to solve, we may want `\"   test\"` and `\"test\"` to be closer than the distance between `\"test\"` and `\"texts\"`. On that note, let's explore the performance of three other edit distance measures: the [Hamming Distance](https://en.wikipedia.org/wiki/Hamming_distance), [Needleman Wunsch](https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm), and [Smith Waterman](https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 0\n",
      "\"test\" and \"texts\" is: 2\n",
      "\"test\" and \"   test\" is: 6\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "levenshtein distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 0\n",
      "\"test\" and \"texts\" is: 2\n",
      "\"test\" and \"   test\" is: 3\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "needleman_wunsch distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 4.0\n",
      "\"test\" and \"texts\" is: 2.0\n",
      "\"test\" and \"   test\" is: 1.0\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "smith_waterman distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 4\n",
      "\"test\" and \"texts\" is: 2.0\n",
      "\"test\" and \"   test\" is: 4.0\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distance_measures = ['hamming',  'levenshtein', 'needleman_wunsch', 'smith_waterman']\n",
    "\n",
    "for method in distance_measures:\n",
    "    print(method + \" distance:\")\n",
    "    print('--------------------------------------------------------')\n",
    "    GetDistance('test'      ,'test'      ,method)\n",
    "    GetDistance('test'      ,'texts'     ,method)\n",
    "    GetDistance('test'      ,'   test'   ,method)\n",
    "    print('--------------------------------------------------------')\n",
    "    print('\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's step through these results together. First, we notice that the Hamming distance between `\"test\"` and `\"   test\"` is 6, which is even higher than what we observed for the Levenshtein distance! To understand why, we can refer to the definition of the [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance): \"The Hamming distance between two strings is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other.\" Given that there are so many white space characters, and that Hamming is only concerned with substitutions, it makes sense that the hamming distance would be higher than the levenshtein distance!\n",
    "\n",
    "At first glance, the results of the [Needleman Wunch](https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm) and [Smith Waterman](https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm) methods seem suprising. Why is the distance between \"test\" and \"test\" 4? Shouldn't it be 0? Once again, the answer here resides in understanding the definition of the \"distance\" metric. Both of these algorithms are actually providing an **alignment score** between the two sequences. \n",
    "\n",
    "Of course, we may also notice that the scores between the two alignment methods are quite different; the reason for this difference has to do with what these alignment scores are exactly measuring: the Smith–Waterman algorithm is designed to find the *segments* in two sequences that have maximal similarities while the Needleman–Wunsch algorithm is designed for alignment between two complete sequences. Let's explore these two methods to make this distinction a little more clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needleman_wunsch distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 4.0\n",
      "\"test\" and \" test\" is: 3.0\n",
      "\"test\" and \"  test\" is: 2.0\n",
      "\"test\" and \"   test\" is: 1.0\n",
      "\"test\" and \"    test\" is: 0.0\n",
      "\"test\" and \"     test\" is: -1.0\n",
      "\"test\" and \"      test\" is: -2.0\n",
      "\"test\" and \"       test\" is: -3.0\n",
      "\n",
      "\n",
      "smith_waterman distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 4\n",
      "\"test\" and \" test\" is: 4.0\n",
      "\"test\" and \"  test\" is: 4.0\n",
      "\"test\" and \"   test\" is: 4.0\n",
      "\"test\" and \"    test\" is: 4.0\n",
      "\"test\" and \"     test\" is: 4.0\n",
      "\"test\" and \"      test\" is: 4.0\n",
      "\"test\" and \"       test\" is: 4.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distance_measures = ['needleman_wunsch', 'smith_waterman']\n",
    "for method in distance_measures:\n",
    "    print(method + \" distance:\")\n",
    "    print('--------------------------------------------------------')\n",
    "    GetDistance('test','test',method)\n",
    "    GetDistance('test',' test',method)\n",
    "    GetDistance('test','  test',method)\n",
    "    GetDistance('test','   test',method)\n",
    "    GetDistance('test','    test',method)\n",
    "    GetDistance('test','     test',method)\n",
    "    GetDistance('test','      test',method)\n",
    "    GetDistance('test','       test',method)\n",
    "    print('\\n')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the Needleman Wunsh method grows more negative as the absolute difference between the two strings increases. In this sense, the Needleman-Wunsch algorithm is similar to the Levenshtein distance algorithm; the difference being that Levenshtein uses a static penalty cost to any mismatched letters while the Needleman-Wunsch algorithm gives weights to matches and mismatches differently. The Smith Waterman method distance remains 4 because the substring `test` overlaps in both strings regardless of the whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token-Level distance\n",
    "All of our edit distance work up until now has focused on character-level distances and alignment but depending on the problem we want to solve, we may care more about distance at the token level. Fortunately, there are several distance metrics that measure similarity between strings at the token level. One such measure is the [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index), which is the size of the intersection of the token sets divided by the size of the union of the sample sets. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard distance:\n",
      "--------------------------------------------------------\n",
      "\"The fish was delish, and it made quite a dish\" and \"The fish on the dish was made quite delicious!\" is: 0.7843137254901961\n",
      "\"The fish was delish, and it made quite a dish\" and \"Joe eats crabs, crabs do not taste good.\" is: 0.39344262295081966\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distance_measures = ['jaccard']\n",
    "text1 = \"The fish was delish, and it made quite a dish\"\n",
    "text2 = \"The fish on the dish was made quite delicious!\"\n",
    "text3 = \"Joe eats crabs, crabs do not taste good.\"\n",
    "for method in distance_measures:\n",
    "    print(method + \" distance:\")\n",
    "    print('--------------------------------------------------------')\n",
    "    GetDistance(text1      ,text2      ,method)\n",
    "    GetDistance(text1      ,text3      ,method)\n",
    "    print('--------------------------------------------------------')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be covering all the edit distance functions of the NLP community in great detail, but hopefully this is enough to whet your appetite, and allow you to dig into some of the methods on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 3:\n",
    "#### Worth 1/5 Points\n",
    "#### A. Towards a More Nuanced Edit Distance Cost Metric\n",
    "As we've eluded to here and in the lectures, one deficiency of the Levenshtein cost metric is the assumption that substitutions, edit, and deletion costs are all equal. Your goal for this learning exercise is to modify the cost structure of the `LevenshteinDistanceDP` method shown below so that is assigns 1/2 of the normal penalty for any edits that involve whitespace characters (FYI, the implementation shown below was originally written by [Ahmed Gad](https://blog.paperspace.com/author/ahmed/), [here](https://blog.paperspace.com/implementing-levenshtein-distance-word-autocomplete-autocorrect/#:~:text=The%20Levenshtein%20distance%20is%20a,representing%20the%20distance%20between%20them.&text=In%20this%20tutorial%20the%20Levenshtein,using%20the%20dynamic%20programming%20approach.)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def levenshteinDistanceDP(token1, token2):\n",
    "    distances = np.zeros((len(token1) + 1, len(token2) + 1))\n",
    "\n",
    "    for t1 in range(len(token1) + 1):\n",
    "        distances[t1][0] = t1\n",
    "\n",
    "    for t2 in range(len(token2) + 1):\n",
    "        distances[0][t2] = t2\n",
    "        \n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        for t2 in range(1, len(token2) + 1):\n",
    "            if (token1[t1-1] == token2[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "            \n",
    "            elif(token1[t1-1] != token2[t2-1]):\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if token1[t1-1]==\" \" and token2[t2-1]!=\" \":\n",
    "                    if (a <= b  and a <= c):\n",
    "                        distances[t1][t2] = a + 0.5\n",
    "                    elif (b <= a and b <= c):\n",
    "                        distances[t1][t2] = b + 0.5\n",
    "                    else:\n",
    "                        distances[t1][t2] = c + 0.5\n",
    "                        \n",
    "                elif token2[t2-1]==\" \" and token1[t1-1]!=\" \":\n",
    "                    if (a <= b  and a <= c):\n",
    "                        distances[t1][t2] = a + 0.5\n",
    "                    elif (b <= a and b <= c):\n",
    "                        distances[t1][t2] = b + 0.5\n",
    "                    else:\n",
    "                        distances[t1][t2] = c + 0.5\n",
    "                        \n",
    "                elif token2[t2-1]==\" \" and token1[t1-1]==\" \":\n",
    "                    if (a <= b  and a <= c):\n",
    "                        distances[t1][t2] = a + 0.5\n",
    "                    elif (b <= a and b <= c):\n",
    "                        distances[t1][t2] = b + 0.5\n",
    "                    else:\n",
    "                        distances[t1][t2] = c + 0.5         \n",
    "                              \n",
    "                else:\n",
    "\n",
    "                    if (a <= b  and a <= c):\n",
    "                        distances[t1][t2] = a + 1\n",
    "                    elif (b <= a and b <= c):\n",
    "                        distances[t1][t2] = b + 1\n",
    "                    else:\n",
    "                        distances[t1][t2] = c + 1\n",
    "                            \n",
    "                        \n",
    "                        \n",
    "\n",
    "    return distances[len(token1)][len(token2)]\n",
    "\n",
    "\n",
    "\n",
    "levenshteinDistanceDP('  test', 'test')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The code does not work when whitespaces put at the begining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "# Part 4: N-Gram language Models\n",
    "In this section, we'll be covering n-gram language models which, as the name implies, are language models that use n-grams!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "An n-gram is a contiguous sequence of n items from a given sample of text or speech. Let's begin this section by defining a function that generates n-grams from our text data using the `nltk` library and using it to extract n-grams for n ranging from 1-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word-Level:\n",
      "1-gram:  ['a', 'fish', 'keeps', 'for', 'a']\n",
      "2-gram:  ['a fish', 'fish keeps', 'keeps for', 'for a', 'a day']\n",
      "3-gram:  ['a fish keeps', 'fish keeps for', 'keeps for a', 'for a day', 'a day ;']\n",
      "4-gram:  ['a fish keeps for', 'fish keeps for a', 'keeps for a day', 'for a day ;', 'a day ; a']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# an ngram extractor \n",
    "def extract_word_ngrams(data, num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# some example text\n",
    "data = \"\"\"a fish keeps for a day; a fish keeps well if you put it in a cold place. \n",
    "          a fish keeps best if you put it in the fridge. If you’ll be having fish, \n",
    "          have it with an apple because one apple a day keeps the doctor away! \n",
    "          You know they also say that a day keeps coming.\"\"\"\n",
    "\n",
    "# printing the first 5 tokens after gramification\n",
    "print(\"\\nWord-Level:\")\n",
    "print(\"1-gram: \", extract_word_ngrams(data, 1)[0:5])\n",
    "print(\"2-gram: \", extract_word_ngrams(data, 2)[0:5])\n",
    "print(\"3-gram: \", extract_word_ngrams(data, 3)[0:5])\n",
    "print(\"4-gram: \", extract_word_ngrams(data, 4)[0:5])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the n-gram extractor I use above leverages the `nltk` tokenizer that we discussed earlier in this assignment. Also, that sample data I showed above should look familiar... As we discussed in the lectures, n-grams can be generated at whatever level of resolution we need for the task at hand; we also don't need any fancy libraries to generate them! If we were interested in extracting n-grams at the character level for instance, we could do that in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Character-Level:\n",
      "1-gram:  ['a', ' ', 'f', 'i', 's']\n",
      "2-gram:  ['a ', ' f', 'fi', 'is', 'sh']\n",
      "3-gram:  ['a f', ' fi', 'fis', 'ish', 'sh ']\n",
      "4-gram:  ['a fi', ' fis', 'fish', 'ish ', 'sh k']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# some example text\n",
    "data = \"\"\"a fish keeps for a day; a fish keeps well if you put it in a cold place. \n",
    "          a fish keeps best if you put it in the fridge. If you’ll be having fish, \n",
    "          have it with an apple because one apple a day keeps the doctor away! \n",
    "          You know they also say that a day keeps coming.\"\"\"\n",
    "\n",
    "def extract_char_ngrams(data, num):\n",
    "    return [data[i:i+num] for i in range(len(data))][:(-num-1)]\n",
    "\n",
    "# printing the first 5 tokens after gramification\n",
    "print(\"\\nCharacter-Level:\")\n",
    "print(\"1-gram: \", extract_char_ngrams(data, 1)[0:5])\n",
    "print(\"2-gram: \", extract_char_ngrams(data, 2)[0:5])\n",
    "print(\"3-gram: \", extract_char_ngrams(data, 3)[0:5])\n",
    "print(\"4-gram: \", extract_char_ngrams(data, 4)[0:5])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-Grams\n",
    "While n-grams bring together contiguous sequences of words, skip-grams include grams that skip over certain terms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'fish', 'keeps'),\n",
       " ('a', 'fish', 'for'),\n",
       " ('a', 'fish', 'a'),\n",
       " ('a', 'fish', 'day'),\n",
       " ('a', 'fish', ';'),\n",
       " ('a', 'fish', 'a'),\n",
       " ('a', 'keeps', 'for'),\n",
       " ('a', 'keeps', 'a'),\n",
       " ('a', 'keeps', 'day'),\n",
       " ('a', 'keeps', ';'),\n",
       " ('a', 'keeps', 'a'),\n",
       " ('a', 'for', 'a'),\n",
       " ('a', 'for', 'day'),\n",
       " ('a', 'for', ';'),\n",
       " ('a', 'for', 'a'),\n",
       " ('a', 'a', 'day'),\n",
       " ('a', 'a', ';'),\n",
       " ('a', 'a', 'a'),\n",
       " ('a', 'day', ';'),\n",
       " ('a', 'day', 'a')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some example text\n",
    "data = \"\"\"a fish keeps for a day; a fish keeps well if you put it in a cold place. \n",
    "          a fish keeps best if you put it in the fridge. If you’ll be having fish, \n",
    "          have it with an apple because one apple a day keeps the doctor away! \n",
    "          You know they also say that a day keeps coming.\"\"\"\n",
    "\n",
    "# Creating skip-grams using the NLTK library\n",
    "skip_gram = list(nltk.skipgrams(nltk.word_tokenize(data),n=3,k=5))\n",
    "skip_gram[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Models \n",
    "A language model refers to any method that assign probabilities to strings. Let's try to develop a more concrete understanding of language models by building a simple one together. More specifically, let's build a model that tries to predict the next word in a sentence, given the last word we saw. One approach to accomplish this is to simply count, for each unique word, what words tend to follow it. To do that, let's start by using the `extract_word_ngrams` tool we showed earlier, and collect all the unique words in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'fish', 'keeps', 'for', 'a', 'day', ';', 'a', 'fish', 'keeps', 'well', 'if', 'you', 'put', 'it', 'in', 'a', 'cold', 'place', '.', 'a', 'fish', 'keeps', 'best', 'if', 'you', 'put', 'it', 'in', 'the', 'fridge', '.', 'If', 'you', '’', 'll', 'be', 'having', 'fish', ',', 'have', 'it', 'with', 'an', 'apple', 'because', 'one', 'apple', 'a', 'day', 'keeps', 'the', 'doctor', 'away', '!', 'You', 'know', 'they', 'also', 'say', 'that', 'a', 'day', 'keeps', 'coming', '.']\n",
      "---------------------------------------\n",
      "Vocabulary (n = 40)\n",
      "---------------------------------------\n",
      "['away', 'that', 'they', ';', '.', 'it', 'the', 'cold', 'have', 'put', 'keeps', 'well', 'doctor', 'be', '!', 'll', 'you', 'with', 'one', 'day', 'in', 'apple', 'know', 'a', 'because', 'fridge', 'say', 'for', 'also', 'You', ',', 'If', 'if', 'best', 'an', 'having', 'coming', 'fish', '’', 'place']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# some example text\n",
    "data = \"\"\"a fish keeps for a day; a fish keeps well if you put it in a cold place. \n",
    "          a fish keeps best if you put it in the fridge. If you’ll be having fish, \n",
    "          have it with an apple because one apple a day keeps the doctor away! \n",
    "          You know they also say that a day keeps coming.\"\"\"\n",
    "\n",
    "unigrams = extract_word_ngrams(data,1)\n",
    "print(unigrams)\n",
    "# get the unique words:\n",
    "vocabulary = list(set(unigrams))\n",
    "print('---------------------------------------')\n",
    "print('Vocabulary (n = ' + str(len(vocabulary)) +')')\n",
    "print('---------------------------------------')\n",
    "print(str(vocabulary) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of the words, lets initialize a `JSON` object called `counts` that will keep track of the next word, given the current word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counts = {}\n",
    "\n",
    "for given_word in vocabulary:\n",
    "    counts[given_word] = {}\n",
    "    #print (counts)\n",
    "    for next_word in vocabulary:\n",
    "        counts[given_word][next_word] = 0\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `counts` object is a dictionary of dictionaries. It's convenient to store the count data this way because we can see the counts of the next word, given the current word, by simply providing the count object the current word. Let's take a look at the count of all possible next words, given that the current word is `fish`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Count of next words, given that current word is \"fish\"\n",
      "------------------------------------------------------\n",
      "{'away': 0, 'that': 0, 'they': 0, ';': 0, '.': 0, 'it': 0, 'the': 0, 'cold': 0, 'have': 0, 'put': 0, 'keeps': 0, 'well': 0, 'doctor': 0, 'be': 0, '!': 0, 'll': 0, 'you': 0, 'with': 0, 'one': 0, 'day': 0, 'in': 0, 'apple': 0, 'know': 0, 'a': 0, 'because': 0, 'fridge': 0, 'say': 0, 'for': 0, 'also': 0, 'You': 0, ',': 0, 'If': 0, 'if': 0, 'best': 0, 'an': 0, 'having': 0, 'coming': 0, 'fish': 0, '’': 0, 'place': 0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "print('Count of next words, given that current word is \"fish\"')\n",
    "print('------------------------------------------------------')\n",
    "print(counts['fish'])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the counts are currently empty; that's because we've only just initialized this object and haven't actually counted anything yet! Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(unigrams)-1):\n",
    "    counts[unigrams[i]][unigrams[i+1]] += 1\n",
    "#print(counts)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at that `fish` count again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Count of next words, given that current word is \"well\"\n",
      "------------------------------------------------------\n",
      "{'away': 0, 'that': 0, 'they': 0, ';': 0, '.': 0, 'it': 0, 'the': 0, 'cold': 0, 'have': 0, 'put': 0, 'keeps': 0, 'well': 0, 'doctor': 0, 'be': 0, '!': 0, 'll': 0, 'you': 0, 'with': 0, 'one': 0, 'day': 0, 'in': 0, 'apple': 0, 'know': 0, 'a': 0, 'because': 0, 'fridge': 0, 'say': 0, 'for': 0, 'also': 0, 'You': 0, ',': 0, 'If': 0, 'if': 1, 'best': 0, 'an': 0, 'having': 0, 'coming': 0, 'fish': 0, '’': 0, 'place': 0}\n",
      "\n",
      "\n",
      "------------------------------------------------------\n",
      "Count of next word \"keeps\", given that current word is \"fish\"\n",
      "------------------------------------------------------\n",
      "0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "print('Count of next words, given that current word is \"well\"')\n",
    "print('------------------------------------------------------')\n",
    "print(counts['well'])\n",
    "\n",
    "print('\\n')\n",
    "print('------------------------------------------------------')\n",
    "print('Count of next word \"keeps\", given that current word is \"fish\"')\n",
    "print('------------------------------------------------------')\n",
    "print(counts['well']['keeps'])\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a count of the words, we can convert these to probabilities by simply dividing by the total incidence of next words, for a given current word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the probabilites JSON object\n",
    "probs = {}\n",
    "for given_word in vocabulary:\n",
    "    probs[given_word] = {}\n",
    "    for next_word in vocabulary:\n",
    "        probs[given_word][next_word] = 0\n",
    "\n",
    "# convert the counts to probabilites\n",
    "for key, value in counts.items():\n",
    "    denominator = 0\n",
    "    for key2, value2 in counts[key].items():\n",
    "        denominator += value2\n",
    "\n",
    "    for key2, value2 in counts[key].items():\n",
    "        probs[key][key2] = value2 / denominator\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the probability of the next word being `keeps`, given that the current word is `fish` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "The probability of the next word being keeps,\n",
      "given that the current word is fish:\n",
      "------------------------------------------------------\n",
      "75.0%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "print('The probability of the next word being keeps,') \n",
    "print('given that the current word is fish:')\n",
    "print('------------------------------------------------------')\n",
    "print(str(100*probs['fish']['keeps']) + '%')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it! You have built your first data-driven language model. Congratulations! Now let's get to work doing some fun things with the language model, like simulating new sentences. Let's do that by choosing a word, and then selecting the next word in accordance to the probability given by our language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Given the token :  keeps\n",
      " The next token is :  for\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_next_gram_from_language_model(probs, given_token):\n",
    "    distribution            = list(probs[given_token].values())\n",
    "    #print (distribution)\n",
    "    sample_from_multinomial = np.random.multinomial(1,distribution)\n",
    "    sample_index            = np.where(sample_from_multinomial==1)[0][0]\n",
    "    word_keys               = list(probs[given_token].keys())\n",
    "    next_word               = word_keys[sample_index]\n",
    "    return(next_word)\n",
    "    \n",
    "given_token = \"keeps\"\n",
    "next_token  = sample_next_gram_from_language_model(probs,given_token)\n",
    "\n",
    "print(' Given the token :  ' + given_token)\n",
    "print(' The next token is :  ' + next_token)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a language model built, we can do all sorts of useful things. Including running our model to generate new sentences we've never seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "keeps for a fish keeps coming . a fish , have it in the fridge .\n",
      "keeps coming . a fish keeps best if you put it\n",
      "keeps best if you put it with an apple a fish\n"
     ]
    }
   ],
   "source": [
    "def create_new_sentence(length, seed_token):\n",
    "    tokens = [seed_token]\n",
    "    for i in range(length):\n",
    "        tokens.append(sample_next_gram_from_language_model(probs,tokens[-1]))\n",
    "        #print(tokens)\n",
    "    return tokens\n",
    "\n",
    "      \n",
    "print('---------------------------------------')\n",
    "print(' '.join(create_new_sentence(15, 'keeps')))\n",
    "print(' '.join(create_new_sentence(10, 'keeps')))\n",
    "print(' '.join(create_new_sentence(10, 'keeps')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 4:\n",
    "#### Worth 2/5 Points\n",
    "Now that you have built your first language model, let's turn our attention back to the philosophers. For this final learning exercise, you will be building a tri-gram `Nietzsche language model`, and a tri-gram `Russel Language Model` using the 5 books from each of the philosophers you collected from Project Gutenburg earlier. \n",
    "\n",
    "Generate a conversation between your philosophers models by seeding one of the models with a tri-gram and allow it to run until it generates a terminating character (a '.', '?' or '!'). Then, have the other model generate a response by providing the ending tri-gram of the the first model as the seed tri-gram for the other model. Iterate like this for 15 sentences and print the text.\n",
    "\n",
    "**Note:** You probably learned from the earlier components of this assignment that Nietzsche and Russel have slightly different writing styles (yes, that was an understatement). For this reason, there are likely to be instances where the ending tri-gram from your Nietzsche model does not show up in the Russel language model and vice-versa. Use Laplace smoothing to handle this problem.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "#Bertrand Russell\n",
    "website = ['https://www.gutenberg.org/files/55610/55610-0.txt',\n",
    "           'https://www.gutenberg.org/cache/epub/4776/pg4776.txt',\n",
    "           'https://www.gutenberg.org/files/37090/37090-0.txt',\n",
    "           'https://www.gutenberg.org/cache/epub/690/pg690.txt',\n",
    "           'https://www.gutenberg.org/cache/epub/2529/pg2529.txt']\n",
    "\n",
    "\n",
    "#Friedrich Nietzsche\n",
    "websiteNietzsche = ['https://www.gutenberg.org/cache/epub/19322/pg19322.txt',\n",
    "                    'https://www.gutenberg.org/files/52915/52915-0.txt',\n",
    "                    'https://www.gutenberg.org/files/52190/52190-0.txt',\n",
    "                    'https://www.gutenberg.org/files/37841/37841-0.txt',\n",
    "                    'https://www.gutenberg.org/cache/epub/5652/pg5652.txt']\n",
    "def trainingdata(website):\n",
    "    trainingdata=[]\n",
    "    for i in website:\n",
    "        site=requests.get(i)\n",
    "        texts=(re.sub(\"\\s+\", \" \", site.text)).lower()\n",
    "        trainingdata.append(texts)\n",
    "    for text in  trainingdata:\n",
    "        data =str( ''.join(trainingdata))\n",
    "    return data\n",
    "\n",
    "\n",
    "#print(trainingdata(website))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Russelldat=trainingdata(website)\n",
    "Russell_language_model=extract_word_ngrams(data, 3)\n",
    "\n",
    "data=trainingdata(websiteNietzsche)\n",
    "Nietzsche_language_model=extract_word_ngrams(data, 3)\n",
    "#print(Nietzschetrigram[-20:-1])\n",
    "\n",
    "def seeds(trigram):\n",
    "    for i in trigram:\n",
    "        if not (trigram[-1]).endswith('.' or '!' or '?'):\n",
    "            trigram.pop()\n",
    "    return trigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nietzschetrigram=seeds(Nietzsche_language_model)\n",
    "Nietzschetrigram[-1]\n",
    "Russell_language_model=extract_word_ngrams(data, 3)\n",
    "\n",
    "def conversation (Nietzsche,Russell):\n",
    "    for i in Russell:\n",
    "        conversation=Nietzsche[-1]+':'+i\n",
    "        print(conversation)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "express permission .:﻿the project gutenberg\n",
      "express permission .:project gutenberg ebook\n",
      "express permission .:gutenberg ebook of\n",
      "express permission .:ebook of the\n",
      "express permission .:of the antichrist\n",
      "express permission .:the antichrist ,\n",
      "express permission .:antichrist , by\n",
      "express permission .:, by f.\n",
      "express permission .:by f. w.\n",
      "express permission .:f. w. nietzsche\n",
      "express permission .:w. nietzsche this\n",
      "express permission .:nietzsche this ebook\n",
      "express permission .:this ebook is\n",
      "express permission .:ebook is for\n",
      "express permission .:is for the\n",
      "express permission .:for the use\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conversation(Nietzschetrigram,Russell_language_model[:16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversations(Nietzsche,Russell):\n",
    "    for i in Russell:\n",
    "        for w in Nietzschetrigram:\n",
    "            if Russell.index(i)==Nietzsche.index(w):\n",
    "                conversations= w +\" : \"+ i\n",
    "        print(conversations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Nietzschetrigram=seeds(Nietzsche_language_model)\n",
    "reversedNietzsche=list(reversed(Nietzschetrigram))\n",
    "Russell_language_model=extract_word_ngrams(data, 3)\n",
    "\n",
    "print(conversations(reversedNietzsche,Russell_language_model)[:16])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I guess i Imissunderstood the question. I have read the question many times but I could not understand it throuly specially this part\n",
    "\"Generate a conversation between your philosophers models by seeding one of the models with a tri-gram and allow it to run until it generates a terminating character (a '.', '?' or '!'). Then, have the other model generate a response by providing the ending tri-gram of the the first model as the seed tri-gram for the other model. Iterate like this for 15 sentences and print the text.\"\n",
    "So I did it in two ways.\n",
    "I really appreciate your time and effort for such a great job.\n",
    "Would you please print a sample of your desired output for tasks,It may help us to undestand the question better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color:red\"> Self Assessment </span></h1>\n",
    "Please provide an assessment of how successfully you accomplished the learning exercises in this assignment according to the instruction provided; do not assign yourself points for effort. This self assessment will be used as a starting point when I grade your assignments. Please note that if you over-estimate your grade on a given learning exercise, you will face a 50% penalty on the total points granted for that exercise. If you underestimate your grade, there will be no penalty.\n",
    "\n",
    "* Learning Exercise 1: \n",
    "    0.75/1 points\n",
    "* Learning Exercise 2: \n",
    "    0.25/1 points\n",
    "* Learning Exercise 3:\n",
    "    0.75/1 points\n",
    "* Learning Exercise 4:\n",
    "    1.25/2 points\n",
    "\n",
    "#### Total Grade: \n",
    "3/5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
